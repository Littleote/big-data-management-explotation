import importlib
from pathlib import Path

import duckdb
from pyspark.sql import SparkSession

ZONE = "exploitation"


def extract(*pipes: str):
    """Run pipelines (by default all defined pipelines) from formatted to exploitation"""
    spark: SparkSession = SparkSession.builder.getOrCreate()
    spark.sparkContext.setLogLevel("ERROR")
    formatted = Path("formatted").absolute()
    exploitation = Path(ZONE).absolute()
    if len(pipes) == 0:
        pipes = [
            dir_.stem
            for dir_ in exploitation.iterdir()
            if dir_.is_dir() and (dir_ / "pipeline.py").exists()
        ]
    for pipe in pipes:
        try:
            pipeline = importlib.import_module(f"{ZONE}.{pipe}.pipeline")
            print(f"Excecuting {pipe}'s pipeline...")
            extraction = pipeline.extract(spark, formatted, exploitation)
            if extraction is None:
                print(f"{pipe}'s table already uses the latest commit")
            else:
                print(f"Updating {pipe}'s table...")
                with duckdb.connect(f"{ZONE}/data.db") as conn:
                    conn.sql(
                        f"CREATE OR REPLACE TABLE {pipe} AS \
                            SELECT * \
                            FROM read_parquet('{ZONE}/{pipe}/{extraction}/*.parquet');"
                    )
                print(f"Updated {pipe}'s table to match latest commit")
        except ModuleNotFoundError:
            print(f"Failed to load {pipe}'s pipeline")
        except Exception as e:
            print(f"Failed to run pipeline {pipe} due to: {e}")
