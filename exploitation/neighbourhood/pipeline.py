import os
from pathlib import Path

import pyspark
from pyspark.sql import SparkSession

from utils import Logger


KEY, VALUE = 0, 1
T_SOURCE, T_LOOKUP = 0, 1

TABLE = "neighbourhood"
S_PADRO = "padro_opendata/out"
S_INCOME = "income_opendata/out"


def extract(spark: SparkSession, formatted: Path, exploitation: Path):
    log = Logger(exploitation / TABLE)
    loaded = log.get_log()

    # Check if there is a newer version
    padro = formatted / S_PADRO
    income = formatted / S_INCOME
    current = [
        str(max([os.path.getmtime(file) for file in dir_.iterdir()]))
        for dir_ in [padro, income]
    ]
    has_latest = len(loaded) == len(current) and all(
        [load == curr for load, curr in zip(loaded, current)]
    )

    if has_latest:
        return None

    # Run exploitation pipeline
    out = (
        spark.read.parquet(income.as_posix())
        .rdd.map(lambda x: ((x.district_id, x.neighborhood_id, x.year), x))
        .join(
            spark.read.parquet(padro.as_posix())
            .rdd.map(lambda x: ((x.district_id, x.neighborhood_id, x.year), x.moved))
            .reduceByKey(lambda x, y: (x or 0) + (y or 0))
        )
        .map(
            lambda x: pyspark.sql.Row(
                **(x[VALUE][T_SOURCE].asDict() | {"moved": x[VALUE][T_LOOKUP]})
            )
        )
        .toDF()
    )
    out.write.parquet((exploitation / TABLE / "out").as_posix(), mode="overwrite")
    with log.get_log_file() as log_file:
        print(*current, sep="\n", file=log_file)
    return "out"
