import json
import os
from datetime import datetime as dt
from pathlib import Path

import pyspark
from pyspark.sql import SparkSession

from utils import Logger


KEY, VALUE = 0, 1
DATA, YEAR = 0, 1
MOVE_IN, MOVE_OUT = 0, 1
T_SOURCE, T_LOOKUP = 0, 1

TABLE = "sales"
OUT = "out"
FILE = "predictive"
S_IDEALISTA = "idealista/out"
S_PADRO = "padro_opendata/out"
S_INCOME = "income_opendata/out"


def extract(spark: SparkSession, formatted: Path, exploitation: Path):
    log = Logger(exploitation / TABLE)
    loaded = log.get_log()

    with open(
        exploitation / TABLE / "schema.json", mode="r", encoding="utf-8"
    ) as handler:
        full_schema = pyspark.sql.types.StructType.fromJson(json.load(handler))

    # Check if there is a newer version
    idealista = formatted / S_IDEALISTA
    income = formatted / S_INCOME
    padro = formatted / S_PADRO
    current = [
        str(max([os.path.getmtime(file) for file in dir_.iterdir()]))
        for dir_ in [idealista, income, padro]
    ]
    loaded = loaded[-len(current) :]
    has_latest = len(loaded) == len(current) and all(
        [load == curr for load, curr in zip(loaded, current)]
    )

    if has_latest:
        return None

    # Run exploitation pipeline
    padro_rdd = spark.read.parquet(padro.as_posix()).rdd.cache()

    out = (
        spark.read.parquet(idealista.as_posix())
        .rdd.keyBy(lambda x: (x.district_id, x.neighborhood_id))
        .join(
            # Latest population
            spark.read.parquet(income.as_posix())
            .rdd.map(lambda x: ((x.district_id, x.neighborhood_id), (x, x.year)))
            .reduceByKey(lambda x, y: x if x[YEAR] > y[YEAR] else y)
        )
        .map(
            lambda x: (
                (
                    x[VALUE][T_SOURCE].district_id,
                    x[VALUE][T_SOURCE].neighborhood_id,
                    int(x[VALUE][T_SOURCE].queryDate[:4]),
                ),
                x[VALUE],
            )
        )
        # This two layers are done in series because else it throws an error
        .join(
            # Empadronats entrant
            padro_rdd.map(
                lambda x: ((x.district_id, x.neighborhood_id, x.year), x.moved)
            ).reduceByKey(lambda x, y: (x or 0) + (y or 0))
        )
        .join(
            # Empadronats sortint
            padro_rdd.map(
                lambda x: (
                    (x.dest_district_id, x.dest_neighborhood_id, x.year),
                    x.moved,
                )
            ).reduceByKey(lambda x, y: (x or 0) + (y or 0))
        )
        .map(
            lambda x: pyspark.sql.Row(
                **(
                    x[VALUE][T_SOURCE][T_SOURCE][T_SOURCE].asDict()
                    | {
                        "population": x[VALUE][T_SOURCE][T_SOURCE][T_LOOKUP][DATA].pop,
                        "RFD": x[VALUE][T_SOURCE][T_SOURCE][T_LOOKUP][DATA].RFD,
                        "queryDate": dt.strptime(
                            x[VALUE][T_SOURCE][T_SOURCE][T_SOURCE].queryDate, "%Y_%m_%d"
                        ),
                        "move_in": x[VALUE][T_SOURCE][T_LOOKUP],
                        "move_out": x[VALUE][T_LOOKUP],
                    }
                )
            )
        )
        .toDF(schema=full_schema)
    )
    out.write.parquet((exploitation / TABLE / OUT).as_posix(), mode="overwrite")
    padro_rdd.unpersist()
    with log.get_log_file() as log_file:
        print(*current, sep="\n", file=log_file)
    return OUT, FILE


def reset(exploitation: Path):
    log = Logger(exploitation / TABLE)
    log.clear()
